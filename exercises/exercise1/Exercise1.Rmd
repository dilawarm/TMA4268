---
title: 'Compulsory exercise 1: Group 10'
author: "Domonkos Jozsef Balint, Dilawar Mahmood, Marianne Stornes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2021
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("knitr") #probably already installed
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")  
#install.packages("MASS")
#install.packages("class")
#install.packages("pROC")
#install.packages("plotROC")
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(class)
library(pROC)
library(plotROC)
```

# Assignment 1

## Problem 1 

### a)
Finding the expected value of $\tilde{\beta}$

$E(\tilde{\beta}) = E[(X^{T}X+\lambda I)^{-1}X^{T}Y]$

$E(\tilde{\beta}) = (X^{T}X+\lambda I)^{-1}X^{T}E(Y)$

$E(Y)=E(f(X)+\varepsilon) = E(f(X))+E(\varepsilon) = X\beta$

$E(\tilde{\beta}) = (X^{T}X+\lambda I)^{-1}X^{T}X\beta$

Finding the variance-covariance matrix of $\tilde{\beta}$

$Var(\tilde{\beta}) = (X^{T}X+\lambda I)^{-1}X^{T}X Var(\hat{\beta}) [(X^{T}X+\lambda I)^{-1}X^{T}X]^{T}$

$Var(\tilde{\beta}) = (X^{T}X+\lambda I)^{-1}X^{T}X \sigma ^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X+\lambda I)^{-1}$

$Var(\tilde{\beta}) = \sigma^{2} (X^{T}X+\lambda I)^{-1}X^{T}X (X^{T}X+\lambda I)^{-1}$

### b)

Finding the expected value for $\tilde{f}(x_0)$

$f(x_{0})=x_{0}^{T}\tilde{\beta}$

$E[f(x_{0})]= x_{0}E[\tilde{\beta}]$

Finding the variance for  $\tilde{f}(x_0)$

$Var(f(x_{0})) = x_{0}^{T}Var(\tilde{\beta})x_{0}$


### c)
Finding the MSE at $x_0$,

$Var(f(x_{0})) = x_{0}^{T}Var(\tilde{\beta})x_{0}$

$E[(y_{0}-\tilde{f}(x_{0}))^{2}] = [E(\tilde{f}(x_{0})-f(x_{0}))]^{2}+Var(\tilde{f}(x_{0}))+Var(\varepsilon)$

$E[(y_{0}-\tilde{f}({x}_{0}))^{2}] = \sigma^{2} + x_{0}^{T}Var(\tilde{\beta})x_{0}+ [f(\mathbf{x}_{0})-E[\hat{f}(x_{0})]]^2$


### d)
Preliminary set up 

```{r, eval=TRUE, echo=TRUE}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m"  # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X = values$X
dim(X)
x0 = values$x0
dim(x0)
beta = values$beta
dim(beta)
sigma = values$sigma
sigma
```


Inserting our result from the previous tasks into value:

```{r, eval=TRUE, echo=TRUE}
library(ggplot2)
bias = function(lambda, X, x0, beta) {
    p = ncol(X)
    value = (t(x0)%*%beta - t(x0)%*%solve(t(X)%*%X+lambda*diag(p))%*%t(X)%*%X%*%beta)^2
    return(value)
}
lambdas = seq(0, 2, length.out = 500)
BIAS = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) BIAS[i] = bias(lambdas[i], X, x0, beta)
dfBias = data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) + geom_line(color = "red") + xlab(expression(lambda)) + 
    ylab(expression(bias^2))
```

Here we see the bias is a bit higher in the beginning, decreases and then increases quickly. When $\lambda$ is around 0.5, the bias is at the lowest. We can see that the higher $\lambda$ goes, the more bias we get, because the model punishes many covariates.

### e)

```{r, eval=TRUE, echo=TRUE}
variance = function(lambda, X, x0, sigma) {
    p = ncol(X)
    inv = solve(t(X) %*% X + lambda * diag(p))
    value = sigma^2%*%t(x0)%*%inv%*%t(X)%*%X%*%inv%*%x0
    return(value)
}
lambdas = seq(0, 2, length.out = 500)
VAR = rep(NA, length(lambdas))
for (i in 1:length(lambdas)) VAR[i] = variance(lambdas[i], X, x0, sigma)
dfVar = data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var)) + geom_line(color = "green4") + xlab(expression(lambda)) + 
    ylab("variance")
```

The variance gets lower as $\lambda$ increases. This could imply overfitting when $\lambda$ gets too high

### f)

```{r, eval=TRUE, echo=TRUE}
exp_mse = rep(sigma^2, 500) + BIAS + VAR
lambdas[which.min(exp_mse)]
```

This is the value that minimizes MSE.

```{r, eval=TRUE, echo=TRUE}
dfAll = data.frame(lambda = lambdas, bias = BIAS, var = VAR, exp_mse = exp_mse)
ggplot(dfAll) + geom_line(aes(x = lambda, y = exp_mse), color = "blue") + geom_line(aes(x = lambda, 
    y = bias), color = "red") + geom_line(aes(x = lambda, y = var), color = "green4") + 
    xlab(expression(lambda)) + ylab(expression(E(MSE)))
```

We see that the bump in the beginning for the bias, is so small compared to the variance, so it can pretty much be ignored. $\lambda = 0$ is normal OLS, so we do get a different result when $\lambda \approx 1$

## Problem 2

During Problem 2 we considered a result significant, if its p-, or f-value is smaller than 0.05 .

### a)

Preliminary set up:
```{r, eval=TRUE, echo=TRUE}
# read file
id <- "1yYlEl5gYY3BEtJ4d7KWaFGIOEweJIn__"  # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = T)
```

Creating the tables

Number of deceased and non deceased
```{r, eval=TRUE, echo=TRUE}
table(d.corona$deceased)
```

Number of males and females in each country
```{r, eval=TRUE, echo=TRUE}
table(d.corona[c("country", "sex")])
```

Number of deceased and non deceased for each sex
```{r, eval=TRUE, echo=TRUE}
table(d.corona[c("sex", "deceased")])
```

Number of deceased and non deceased for each sex in France
```{r, eval=TRUE, echo=TRUE}
table(d.corona[d.corona[,"country"] == "France",][c("sex", "deceased")])
```

### b)

Preliminary set up
```{r, eval=TRUE, echo=TRUE}
d.corona$sex <- as.factor(d.corona$sex)
d.corona$country <- as.factor(d.corona$country)
d.corona$deceased <- as.factor(d.corona$deceased)
model.fit = glm(deceased ~ ., data = d.corona, family = "binomial")
```

We fitted a logistic regression model as this is a binary classification task.

```{r, eval=TRUE, echo=TRUE}
summary(model.fit)
```

#### i) 
We predicted the output with the model we fitted earlier. The probability is 0.1084912, as shown below. 

```{r, eval=TRUE, echo=TRUE}
newdata = data.frame(matrix(ncol=3, nrow=0))
colnames(newdata) = names(d.corona)[2:4]
newdata[1,] = list("male", 75, "Korea")
```

```{r, eval=TRUE, echo=TRUE}
predict(model.fit, newdata=newdata, type="response")
```

Set up of ii) - iv)
```{r, eval=TRUE, echo=TRUE}
summary(model.fit)$coef
```

#### ii) 
Yes, since in the model, female is the reference, and the estimate for sexmale is positive. Also we can see, from the p-value of sexmale that this is a significant result.

#### iii) 
F-test of the different coefficients:
```{r, eval=TRUE, echo=TRUE}
anova(model.fit, test="Chisq")
```

As we can see from the f-value (0.014), the country coefficient is significant, and after examining its different values (-0.41, -1.34, -0.77) we can determine that it has a non-negligible influence on the output. 

#### iv) 
$e^{0.02713421\cdot 10}\approx 1.312$ as the coefficient for age is 0.27, and the odds changing can be quantified according the formula $e^{coefficient*change}$.

### c)

#### i) 

```{r, eval=TRUE, echo=TRUE}
model.fit = glm(deceased ~ sex + age + country + age:sex, data = d.corona, family = 'binomial')
summary(model.fit)$coef
```

The coefficient of sexmale:age is smaller with an order of magnitude than coefficient age, and from its p-value (0.91) we can determine that this not has a significant influence to the output. Hence age is not a greater risk factor for males than for females.

#### ii) 
```{r, eval=TRUE, echo=TRUE}
model.fit = glm(deceased ~ sex + age + country + age:country, data = d.corona, family = 'binomial')
summary(model.fit)$coef
```

Since age:countryindonesia is negative, and the baseline number is France, we could conclude that age is a higher risk for French population, than Indonesian. However if we calculate the f-value of the interaction coefficient (0.091), we can see that there is a significant chance, that age:country interaction does not have an effect on the output. Overall according to our basic assumption related to f-value, we can conclude, that age is not a greater risk factor for French population than for the Indonesian population. 

```{r, eval=TRUE, echo=TRUE}
anova(model.fit, test="Chisq")
```

### d)
TRUE, TRUE, TRUE, FALSE 

## Problem 3

### a)

#### i) 

$$\begin{aligned}
\log\left(\frac{p_i}{1-p_i}\right)&=\log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}{1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}\right)\\
&=\log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}{\frac{1+e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{1+e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}\right)\\
&=\log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}{\frac{1}{ 1+ 
e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}}}\right)\\
&=\log\left(e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}}\right)\\
&=\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \dots + \beta_7 x_{i7}
\end{aligned}$$

As we can see, the logit function is a linear function of the covariates.

#### ii)
Using the code provided in the problem to fit the `logReg` model.

```{r, eval=TRUE, echo=TRUE}
# read file
id <- "1i1cQPeoLLC_FyAH0nnqCnnrSBpn05_hO"  # google file ID
diab <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
t = MASS::Pima.tr2
train = diab$ctrain
test = diab$ctest
```

```{r, eval=TRUE, echo=TRUE}
logReg = glm(diabetes ~ ., data = train, family = "binomial")
summary(logReg)
```
Calculating confusion table for the `logReg` model:
```{r, eval=TRUE, echo=TRUE}
logReg.prediction = predict(logReg, newdata = test[-1], type = 'response')
confusion_table = table(predicted = ifelse(logReg.prediction>0.5, 1, 0), true = test$diabetes)
confusion_table
```

Sensitivity
```{r, eval=TRUE, echo=TRUE}
sensitivity = confusion_table[4]/sum(confusion_table[,2])
sensitivity
```

Specificity
```{r, eval=TRUE, echo=TRUE}
specificity = confusion_table[1]/sum(confusion_table[,1])
specificity
```

### b)

#### i) 
$\pi_k$ is the prior probability for class $k$, where $k\in\{\text{diabetes not present}, \text{diabetes present}\}=\{0,1\}$. This means that $\pi_0=\frac{\text{\# diabetes not present}}{\text{\# observations}},$ and $\pi_1=\frac{\text{\# diabetes present}}{\text{\# observations}}.$

$\boldsymbol{\mu}_k$ is the mean value for class $k$. This value is estimated by calculating the average of the observations for each class $k$.

$\boldsymbol{\Sigma}$ is the covariance matrix. The covariance matrix is the same for both classes, and this is because the covariance matrices for each class $k$ are pooled.

$f_k(x)$ is the multivariate distribution function for class $k$ with a mean $\boldsymbol{\mu}_k$ and a covariance matrix $\boldsymbol{\Sigma}$. We can write $f_k(x)=P(\boldsymbol X=\boldsymbol x|Y=k)$, which tells us the probability for finding an $\boldsymbol x$ given that a person has diabetes or not.

#### ii) 
LDA (Linear Discriminant Analysis) is used when a linear boundary is required between classifiers and QDA (Quadratic Discriminant Analysis) is used to find a non-linear boundary between classifiers. In QDA, the covariate matrices are not equal for each class.

Fitting a LDA model to the training data:
```{r, eval=TRUE, echo=TRUE}
lda.model = lda(diabetes ~ ., data=train)
lda.prediction = predict(lda.model, newdata = test[-1], type = 'response')
table(predicted = lda.prediction$class, true = test$diabetes)
```
Fitting a QDA model to the training data:
```{r, eval=TRUE, echo=TRUE}
qda.model = qda(diabetes ~ ., data=train)
qda.prediction = predict(qda.model, newdata = test[-1], type = 'response')
table(predicted = qda.prediction$class, true = test$diabetes)
```
As we can see from the confusion tables, the QDA model has a higher sensitivity than the LDA model. $\frac{30}{77}$ for LDA, and $\frac{32}{77}$ for QDA. 

On the other hand, the LDA model has a higher specificity than the QDA model. $\frac{138}{155}$ for LDA, and $\frac{131}{155}$ for QDA.

If we only look at the correct classifications, LDA has a test accuracy of $\frac{138+47}{138+17+30+47}\approx0.80$, while QDA has a test accuracy of $\frac{131+45}{131+24+32+45}\approx0.76,$ so LDA performs a bit better in terms of test accuracy.

The tables for both of the models are relatively equal, so QDA does not perform better than LDA.

### c)

#### i) 
How a new observations is classified in the KNN approach:

* Calculate the Euclidean distance from the new observation to the rest of the observations in the dataset.
* Look at the $k$ closest points, and ignore the other points.
* Choose the class which is the majority among those $k$ points.

#### ii) 

Here we would choose the tuning parameter $k$ by fitting the model for different values of $k$, and calculating the misclassification error when tested on a validation dataset. Then we would choose the $k$ which gives the lowest misclassification error. If there are multiple $k$'s that give approximately the same misclassification error, we would choose the lowest $k$, so that the model is easy to interpret.

#### iii)
Using $k=25$ to classify the presence of diabetes in the testing data with a KNN model:
```{r, eval=TRUE, echo=TRUE}
knnMod = knn(train=train[-1] , test=test[-1], cl=train$diabetes, k=25, prob=T)
confusion_table = table(predicted=knnMod, true=test$diabetes)
confusion_table
```
Calculating the sensitivity and specificity using the confusion table:
```{r, eval=TRUE, echo=TRUE}
sensitivity = confusion_table[4]/sum(confusion_table[,2])
sensitivity
```

```{r, eval=TRUE, echo=TRUE}
specificity = confusion_table[1]/sum(confusion_table[,1])
specificity
```

### d)
Producing the ROC curves for all 4 models:

```{r, eval=TRUE, echo=TRUE}
knn.roc = roc(test$diabetes, ifelse(knnMod == 0, 1 - attributes(knnMod)$prob, attributes(knnMod)$prob))
lda.roc = roc(test$diabetes, lda.prediction$posterior[,2])
qda.roc = roc(test$diabetes, qda.prediction$posterior[,2])
logReg.roc = roc(test$diabetes, logReg.prediction)
```

Calculating the AUC for all 4 models:
```{r, eval=TRUE, echo=TRUE}
pROC::ggroc(list("KNN"=knn.roc, "LDA"=lda.roc, "QDA"=qda.roc, "logReg"=logReg.roc))
```

Calculating the AUC for all 4 models:
```{r, eval=TRUE, echo=TRUE}
auc(knn.roc)
auc(lda.roc)
auc(qda.roc)
auc(logReg.roc)
```

AUC tells us that the LDA is the best performing better. If the task is to create an interpretable model, we would choose logistic regression because the model estimates the logarighm of the odds ratio for the different covariates, and is easy to convert to probabilities, thus more interpretable. We also have a binary class setting here, so logistic regression is an appropriate model to use.

## Problem 4

### a)
Show that for linear regression the LOOCV statistic can be computed by the following formula: 

$$CV = \frac{1}{N}\sum_{i=1}^N \left( \frac{y_i-\hat{y}_i}{1-h_i} \right)^2$$

**Proof**

First we calculate $\hat{y}_{(-i)}$:
$$\hat{y}_{(-i)}=x_i^T\hat{\beta}_{(-i)}=x_i^T(X_{(-i)}^T X_{(-i)})^{-1}X_{(-i)}^TY_{(-i)}=$$

after using hint 2, and 2.1:

$$=x_i^T(XX^T-x_ix_i^T)^{-1}(X^TY-x_iy_i^T)=$$

Changing the inverse part according to Sherman-Morrison formula:

$$=x_i^T((X^TX)^{-1}+\frac{(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}}{1-x_k^T(X^TX)^{-1}x_i}(X^TY-x_iy_i^T)=$$

We compute the products:

$$=x_i^T(X^TX)^{-1}X^TY-x_i^T(X^TX)^{-1}x_iy_i+\frac{x_i^T(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}X^TY-x_i^T(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}x_iy_i}{1-x_i^T(X^TX)^{-1}x_i}=$$

plug into the equation $h_i$ and $\hat{\beta}$ where we can:

$$=x_i^T\hat{\beta} - h_i y_i + \frac{h_ix_i^T\hat{\beta}-h_ih_iy_i}{1-h_i} = $$

plug into the equation $\hat{y}_i$ where we can:

$$= \frac{(1-h_i)\hat{y}_i - (1-h_i)h_iy_i^T + h_i\hat{y}_i-h_ih_iy_i^T}{1-h_i} = \frac{\hat{y}_i-h_iy_i}{1-h_i}$$

as $y_i$ is scalar. 

According the results shown above:
$$y_i-\hat{y}_{(-i)} = y_i - \frac{\hat{y}_i-h_iy_i}{1-h_i} = \frac{y_i - \hat{y}_i}{1-h_i}$$

If we plug in this result into the cross-validation formula we prove the problem. 

### b)
FALSE, TRUE, TRUE, FALSE

## Problem 5

### a)
Initial setup

```{r, eval=TRUE, echo=TRUE}
id <- "19auu8YlUJJJUsZY8JZfsCTWzDm6doE7C" # google file ID
d.bodyfat <-read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = T)
```

Finding the $R^2$

```{r, eval=TRUE, echo=TRUE}
model.fit = lm(bodyfat ~ age + weight + bmi, data = d.bodyfat)
summary(model.fit)$r.squared
```

### b)

#### i)

Generating bootstrap samples and saving the $R^2$ in an array.

```{r, eval=TRUE, echo=TRUE}
n = dim(d.bodyfat)[1]

set.seed(4268)

k = 1000
b_estimates = rep(NA, k)

for(i in 1:k){
    b_sample = sample(1:n, n, replace = TRUE)
    b_lm = lm(bodyfat ~ age + weight + bmi, data = d.bodyfat[b_sample, ])
    b_estimates[i] = summary(b_lm)$r.squared
}
```

#### ii)
Plotting the $R^2$'s

```{r, eval=TRUE, echo=TRUE}
library(ggplot2)

data =data.frame(b_estimates = b_estimates, norm_den =dnorm(b_estimates, 
                                                            mean(b_estimates), sd(b_estimates)))

ggplot(data)+ geom_histogram(aes(x = b_estimates, y = ..density..), fill = "grey80",color = "black") + 
  geom_line(aes(x = b_estimates, y = norm_den), color = "red")+theme_minimal()
```

#### iii)

Deriving the standard error and the  95% confidence interval

```{r, eval=TRUE, echo=TRUE}

sd(b_estimates)
quantile(b_estimates, c(.025, .975))
```

#### iv)

First of all, we can see that the distribution is normal, and that is a consequence of the central limit theorem because we are using a large sample size. The 95% confidence interval tells us that there is a probability of 95% that $R^2$ is between 0.51 and 0.65.